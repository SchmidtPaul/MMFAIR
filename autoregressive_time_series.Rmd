---
title: "Repeated Measures in Time"
output: 
  html_document:
    includes:
      in_header: header.html
      after_body: footer.html	
params:
  hilang:
    - sas					 
---

```{r echo=FALSE, message=FALSE, warning=FALSE, purl=FALSE}
# markdown options
options(knitr.kable.NA = '') # show NA as empty cells in output tables
pacman::p_load(kableExtra, formattable, htmltools) # packages for better formatting tables for html output
```

```{r, child="_hilang_setup.Rmd"}
```

```{r}
# packages
pacman::p_load(dplyr, purrr, tibble, tidyr, stringr, # data handling
               ggplot2, viridis,            # plot
               nlme, lme4, glmmTMB, sommer, # mixed modelling
               AICcmodavg, broom.mixed)     # mixed model extractions

# data
dat <- agriTutorial::sorghum %>%
  rename(block = Replicate, plot = factplot) %>% 
  dplyr::select(y, variety, block, plot, factweek, varweek) %>% 
  as_tibble()
```
```{r, echo=FALSE, purl=FALSE}
dat %>%
  kable(escape = FALSE) %>% 
  kable_styling(bootstrap_options = c("bordered", "hover", "condensed", "responsive"), 
                full_width = FALSE) %>% 
  scroll_box(height = "200px")
```

<br/>

# Motivation

<div class = "row"> <div class = "col-md-6">

The example in this chapter is taken from [*Example 4* in Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} (see also the [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"}). It considers data from a sorghum trial laid out as a randomized complete `block` design (5 blocks) with `variety` (4 sorghum varities) being the only treatment factor. Thus, we have a total of 20 `plot`s. It is important to note that our response variable (`y`), **the leaf area index, was assessed in five consecutive weeks on each plot** starting 2 weeks after emergence. Therefore, the dataset contains a total of 100 values and what we have here is longitudinal data, *a.k.a.* repeated measurements over time, *a.k.a.* a time series analysis.

As [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} put it: *"the week factor is not a treatment factor that can be randomized. Instead, repeated measurements are taken on each plot on five consecutive occasions. Successive measurements on the same plot are likely to be serially correlated, and this means that for a reliable and efficient analysis of repeated-measures data we need to take proper account of the serial correlations between the repeated measures ([Piepho, Büchse & Richter, 2004](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-037X.2004.00097.x){target="_blank"}; [Pinheiro & Bates, 2000](http://library.mpib-berlin.mpg.de/toc/z2008_18.pdf){target="_blank"})."*


</div> <div class = "col-md-6">

```{r, echo=FALSE, fig.height=3.5, fig.width=4, purl=FALSE}
ggplot(data = dat, aes(
  y = y,
  x = factweek,
  group = variety,
  color = variety
)) +
  geom_point(alpha = 0.5, size = 3) +
  scale_color_viridis(option = "D",
                      discrete = TRUE,
                      name = "Variety") +
  scale_y_continuous(
    name = "Leaf area index",
    limits = c(0, 6.5),
    expand = c(0, 0),
    breaks = c(0:6)
  ) +
  xlab("Week") +
  theme_bw() +
  theme(legend.position = "bottom")
```

</div> </div>

# Modelling

Our goal is therefore to build a suitable model taking serial correlation into account. In order to do this, we will initially consider the model for a single time point. Then, we extend this model to account for multiple weeks by allowing for week-speficic effects. Finally, we further allow for serially correlated error terms. 

# Sinlge week

When looking at data from a single time point (*e.g.* the first week), we merely have 20 observations from a randomized complete block design with a single treatment factor. It can therefore be analyzed with a simple one-way ANOVA (fixed `variety` effect) for randomized complete block designs (fixed `block` effect):

```{r}
dat.wk1 <- dat %>% filter(factweek == "1") # subset data from first week only

mod.wk1 <- lm(formula = y ~ variety + block,
              data = dat.wk1)
```

We could now go on and look at the ANOVA via `anova(mod.wk1)` and it would indeed not be _wrong_ by simply repeating this for each week. Yet, one may not be satisfied with obtaining multiple ANOVA results - namely one per week. This is especially likeliy in case the results contradict each other, because *e.g.* the variety effects are found to be significant in only two out for five weeks. Therefore, one may want to analyze the entire dataset *i.e.* the multiple weeks jointly.

# Multiple weeks - independent errors {.tabset .tabset-fade .tabset-pills}

Going from the single-week-analysis to jointly analyzing the entire dataset is more than just changing the `data =` statement in the model. This is because *"it is realistic to assume that the treatment effects evolve over time and thus are week-specific. Importantly, we must also allow for the block effects to change over time in an individual manner. For example, there could be fertility or soil type differences between blocks and these could have a smooth progressive or cumulative time-based effect on differences between the blocks dependent on factors such as temperature or rainfall"* [(Piepho & Edmondson, 2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"}. We implement this by taking the model in `mod.wk1` and multipyling each effect with `factweek`. Note that this is also true for the general interecept (µ) in `mod.wk1`, meaning that we would like to include one intercept per week, which can be achieved by simply adding `factweek` as a main effect as well. This leaves us with (fixed) main effects for `factweek`, `variety`, and `block`, as well as the week-specific effects of the latter two `factweek:variety` and `factweek:block`.

## nlme

Since the models in this chapter do not contain any random effects, we make use of `gls()` instead of `lme()`. Furthermore, the above named model `factweek + variety + block + factweek:variety + factweek:block` can be written in a shorter syntax as:

```{r}
mod.iid.nlme <- nlme::gls(model = y ~ factweek * (variety + block),
                          correlation = NULL, # default, i.e. homoscedastic, independent errors
                          data = dat)

# Extract variance component estimates
mod.iid.nlme.VC <- tibble(varstruct = "iid") %>% 
  mutate(sigma    = mod.iid.nlme$sigma) %>% 
  mutate(Variance = sigma^2)
```
```{r, echo=FALSE, purl=FALSE}
mod.iid.nlme.VC %>%
  mutate_if(is.double, round, 3) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(bootstrap_options = c("bordered", "hover", "condensed", "responsive"), 
                full_width = FALSE)
```

## lme4

Since the models in this chapter do not contain any random effects, we cannot use `lmer()` or any other function of the `lme4` package. However, even if there were random effects in our models, the short answer here is that with `lme4` it is **not possible** to fit any variance structures.

More specifically, we can read in an [`lme4` vigniette](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf){target="_blank"}: *"The main advantage of `nlme` relative to `lme4` is a user interface for fitting models with structure in the residuals (various forms of heteroscedasticity and autocorrelation) and in the random-effects covariance matrices (e.g., compound symmetric models).  With some extra effort, the computational machinery of `lme4` can be used to fit structured models that the basic `lmer` function cannot handle (see [Appendix A](https://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf#%5B%7B%22num%22%3A15%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C81%2C733.028%2Cnull%5D){target="_blank"})"*

Michael Clark [puts it as](https://m-clark.github.io/mixed-models-with-R/extensions.html#heterogeneous-variance){target="_blank"} *"Unfortunately, lme4 does not provide the ability to model the residual covariance structure, at least [not in a straightforward fashion](https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html){target="_blank"}"*

**Accordingly, there is no info on this package for this chapter beyond this point.**

<br/>

## glmmTMB

In `glmmTMB()` it is -to our knowledge- not possible to adjust the variance structure of the error.

>Just like in `nlme`, there is a `weights=` argument in `glmmTMB()`. However, to our understanding, they have different functions: 
> 
>In `nlme`, it requires *"an optional `varFunc` object or one-sided formula describing the within-group heteroscedasticity structure"* [(nlme RefMan)](https://cran.r-project.org/web/packages/glmmTMB/glmmTMB.pdf#Rfn.glmmTMB.1){target="_blank"} and we make use of this in the chapter at hand.   
>
>In `glmmTMB`, the [RefMan](https://cran.r-project.org/web/packages/glmmTMB/glmmTMB.pdf#Rfn.glmmTMB.1){target="_blank"} only states *"weights, as in `glm`. Not automatically scaled to have sum 1"*. Following this trail, the [`glm` documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html){target="_blank"} description for the `weights=` argument is *"an optional vector of ‘prior weights’ to be used in the fitting process. Should be NULL or a numeric vector."*.
> Accordingly, it cannot be used to allow for heterogeneous error variances in this package.

We can, however, "*fix the residual variance  to  be  0  (actually  a  small  non-zero  value)*" and therefore "*force variance into the random effects*" [(glmmTMB RefMan)](https://cran.r-project.org/web/packages/glmmTMB/glmmTMB.pdf#Rfn.glmmTMB.1){target="_blank"} via adding the `dispformula = ~ 0` argument. Thus, when doing so, we need to make sure to also add a random term to the model with the desired variance structure. By taking both of these actions, we are essentially mimicing the error (variance) as a random effect (variance). We achieve this by first creating a `unit` column in the data with different entries for each data point. 

Note that doing this may not be necessary for `mod.iid.glmm` with the default homoscedastic, independent error variance structure, but since it will be necessary for the following models with more sophisticated variance structures, we will apply it here, too:

```{r}
dat <- dat %>%
  mutate(unit = 1:n() %>% as.factor) # new column with running number

mod.iid.glmm <- glmmTMB(formula = y ~ factweek * (variety + block) 
                        + (1 | unit),      # add random unit term to mimic error variance
                        dispformula = ~ 0, # fix original error variance to 0
                        REML = TRUE,       # needs to be stated since default = ML
                        data = dat) 

# Extract variance component estimates
mod.iid.glmm.VC <- mod.iid.glmm %>%
  tidy(effects = "ran_pars", scales = "vcov") %>%
  separate(term, sep = "__", into = c("term", "grp"))
```
```{r, echo=FALSE, purl=FALSE}
mod.iid.glmm.VC %>%
  mutate_if(is.double, round, 3) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(bootstrap_options = c("bordered", "hover", "condensed", "responsive"), 
                full_width = FALSE)
```

## sommer

in progress

## SAS

In SAS, there is an actual `subject=` statement that is used to identify the subjects on which repeated measurements are taken.

```{r, eval=FALSE, hilang="sas", purl=FALSE}
proc mixed data=dat;
class variety block factweek plot;
model y=block variety factweek block*factweek variety*factweek /ddfm=kr2;
repeated factweek/sub=plot type=VC rcorr;
run; 
```

# Multiple weeks - autocorrelated errors {.tabset .tabset-fade .tabset-pills}

Note that at this point of the analysis, the model above with an independent, homogeneous error term above is neither the right, nor the wrong choice. It must be clear that *"measurements on the same plot are **likely** to be serially correlated"*. Thus, it should be investigated whether any covariance structure for the error term (instead of the default independence between errors) is more appropriate to model this dataset. It could theoretically be the case that this `mod.iid` is the best choice here, but we cannot confirm this yet, as we have not looked at any alternatives. This is what we will do in the next step.

> One may ask **at what step of the analysis it is best to compare and find the appropriate covariance structure for the error term** in such a scenario. It is indeed here, at this step. As [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} write: *"**Before modelling the treatment effect**, a variance–covariance model needs to be identified for these correlations. This is best done by using a saturated model for treatments and time, i.e., a model that considers all treatment factors and time as qualitative."* 
> 
> <br/>
>
> Note that this *saturated model* is what we have in `mod.iid`. So in short: one should compare variance structures for the error term **before** running an ANOVA / conducting model selection steps. 

Units on which repeated observations are taken are often referred to as subjects. We would now like to allow measurements taken on the same subjects (plot in this case) to be serially correlated, while observations on different subjects are still considered independent. More specifically, we want the errors of the respective observations to be correlated in our model. 

<img src="img\corrvalues.PNG" style="width:60%; margin-right: 10px" align="left">

Take the visualisation on the left depicting a subset of our data. Here, you can see plots 1 and 2 (out of the total of 20 in our dataset) side by side. Furthermore, they are shown for weeks 1-3 (out of the total of 5 in our dataset). Thus, a total of six values are represented, coming from only two subjects/plots, but obtained in three different weeks. The blue arrows represent correlation among errors. For these six measurements, there are six blue arrows, since there are six error pairs that come from the same plot, respectively. The green lines on the other hand represent error pairs that do not come from the same plot and thus are assumed to be independent. Finally, notice that the errors coming from the same plot, but with two instead of just one week between their measurements, are shown in a lighter blue. This is because one may indeed assume a weaker correlation between errors that are further apart in terms of time passed between measurements.

The latter can be achieved by using the maybe most popular correlation structure for repeated measures over time: **first order autoregressive AR(1)**. Please find the section on AR(1) in [our summary on correlation/variance strucutres here.](variance_structures.html){target="_blank"} It should be noted that this correlation structure is useful, if all time points are equally spaced, which is the case here, as there is always exactly one week between consecutive time points.

> There are other possible models (*i.e.* [other variance structures](variance_structures.html){target="_blank"}) for serial correlation of longitudinal data, but for simplicity we will only compare the default `iid` model above with the `ar1` model below. In fact, [*Example 4* in Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} does investigate other models as well and you can find their `nlme` code in the [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"}.

## nlme

In `nlme` we make use of the `correlation =` argument and use the `corAR1` [correlation strucutre class](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.corClasses.1){target="_blank"}. In its syntax, subjects are identified after the `|`.

```{r}
mod.ar1.nlme <- nlme::gls(model = y ~ factweek * (variety + block),
                          correlation = corAR1(form = ~ varweek | plot),
                          data = dat)

# Extract variance component estimates
mod.ar1.nlme.VC <- tibble(varstruct = "ar(1)") %>%
  mutate(sigma    = mod.ar1.nlme$sigma,
         rho      = coef(mod.ar1.nlme$modelStruct$corStruct, unconstrained = FALSE)) %>%
  mutate(Variance = sigma^2,
         Corr1wk  = rho,
         Corr2wks = rho^2,
         Corr3wks = rho^3,
         Corr4wks = rho^4)
```
```{r, echo=FALSE, purl=FALSE}
mod.ar1.nlme.VC %>%
  mutate_if(is.double, round, 3) %>% 
  kable(escape = FALSE) %>% 
  kable_styling(bootstrap_options = c("bordered", "hover", "condensed", "responsive"), 
                full_width = FALSE)
```

## lme4

not possible - see above.

## glmmTMB

```{r}
mod.ar1.glmm <- glmmTMB(formula = y ~ factweek * (variety + block)
                        + ar1(factweek + 0 | plot), # add ar1 structure as random term to mimic error variance
                        dispformula = ~ 0, # fix original error variance to 0
                        REML = TRUE,       # needs to be stated since default = ML
                        data = dat) 

# Extract variance component estimates
mod.iid.glmm.VC <- mod.ar1.glmm %>%
  tidy(effects = "ran_pars", scales ="sdcor") %>%
  separate(term, sep = "__", into = c("term", "grp"))
```
```{r, echo=FALSE, purl=FALSE}
# mod.ar1.glmm %>%
#   mutate_if(is.double, round, 3) %>% 
#   kable(escape = FALSE) %>% 
#   kable_styling(bootstrap_options = c("bordered", "hover", "condensed", "responsive"), 
#                 full_width = FALSE)
```


## sommer

in progress

## SAS

```{r, eval=FALSE, hilang="sas", purl=FALSE}
proc mixed data=dat;
class variety block factweek plot;
model y=block variety factweek block*factweek variety*factweek /ddfm=kr2;
repeated factweek/sub=plot type=ar(1) rcorr;
run; 
```

# More on this 

*"the error terms must now be allowed to be serially correlated. It is important to note that measurements taken on the same observational unit (plot in this case) are serially correlated, but observations on different units can be considered independent because of the randomized allocation of treatments to units. , and in the specification of these models, it is important to suitably identify the subjects. In SAS, for example, subject= option is used for that purpose. In R, subjects are identified after a vertical bar in the argument needed with the respective correlation function or in the random effect specification. Observations on the same subject are serially correlated, whereas observations on different subjects are independent. There are many models for serial correlation of longitudinal data (repeated measurements over time)."*

The larger the distance in time (time lag), the lower is the covariance and correlation. This is depicted in Figure 7 for a few values of the autocorrelation q. This correlation model is useful, if all time points are equally spaced. IDEA: R Shiny zum selbst ausprobieren.